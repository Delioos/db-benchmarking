use dotenv::dotenv;
use native_tls::TlsConnector;
use postgres_native_tls::MakeTlsConnector;
use serde::Deserialize;
use serde_json;
use std::env;
use std::fs::File;
use std::time::Instant;
use tokio_postgres::binary_copy::BinaryCopyInWriter;
use tokio_postgres::types::Type;
use tokio_postgres::{Client, NoTls};

mod error;
mod models;
mod schema;

fn load_json_data<T>(file_path: &str) -> Result<Vec<T>, serde_json::Error>
where
    T: for<'a> Deserialize<'a>,
{
    let file = File::open(file_path).unwrap();
    serde_json::from_reader(file)
}

#[tokio::main]
async fn main() -> error::Result<()> {
    dotenv().ok();

    let database_url = env::var("DATABASE_URL").expect("DATABASE_URL must be set");

    let tls_connector = match TlsConnector::builder().build() {
        Ok(connector) => connector,
        Err(_) => return Err(error::BenchmarkError::TlsError()),
    };

    let postgres_tls_connector = MakeTlsConnector::new(tls_connector);

    let (mut client, connection) =
        match tokio_postgres::connect(&database_url, postgres_tls_connector).await {
            Ok((client, connection)) => (client, connection),
            Err(e) => return Err(error::BenchmarkError::DatabaseError(e)),
        };

    // Spawn the connection future to drive the connection in the background
    tokio::spawn(async move {
        if let Err(e) = connection.await {
            eprintln!("Database connection error: {}", e);
        }
    });

    // Create tables if they don't exist
    match schema::create_tables(&mut client).await {
        Ok(_) => println!("Tables created successfully"),
        Err(e) => return Err(e),
    }

    // Now we can execute a simple statement that just returns its parameter.
    let rows = client.query("SELECT $1::TEXT", &[&"hello world"]).await?;

    // And then check that we got back the same string we sent over.
    let value: &str = rows[0].get(0);
    assert_eq!(value, "hello world");

    let start = Instant::now();
    // Load blocks from JSON file
    let prefix: &str = "../../data/S"; // remove the S to x100 the data size
                                       // let prefix: &str = "../../data/";
    let blocks: Vec<models::Block> = match load_json_data(&format!("{}blocks.json", prefix)) {
        Ok(data) => data,
        Err(e) => {
            eprintln!("Error loading blocks: {}", e);
            Vec::new()
        }
    };

    // Load pools from JSON file
    let pools: Vec<models::Pool> = match load_json_data(&format!("{}pools.json", prefix)) {
        Ok(data) => data,
        Err(e) => {
            eprintln!("Error loading pools: {}", e);
            Vec::new()
        }
    };

    // Load transactions from JSON file
    let transactions: Vec<models::Transaction> =
        match load_json_data(&format!("{}transactions.json", prefix)) {
            Ok(data) => data,
            Err(e) => {
                eprintln!("Error loading transactions: {}", e);
                Vec::new()
            }
        };

    // Load transfers from JSON file
    let transfers: Vec<models::Transfer> =
        match load_json_data(&format!("{}transfers.json", prefix)) {
            Ok(data) => data,
            Err(e) => {
                eprintln!("Error loading transfers: {}", e);
                Vec::new()
            }
        };
    let duration = start.elapsed();
    // Print the loaded data
    println!("Loaded {} blocks", blocks.len());
    println!("Loaded {} transactions", transactions.len());
    println!("Loaded {} transfers", transfers.len());
    println!("Loaded {} pools", pools.len());
    println!("in {:?}", duration);

    // backtest
    // Benchmark Operations
    let start = Instant::now();

    // 1. Bulk Insert Test
    let batch_size = 1000;
    let total_records = 100_000;
    let num_batches = total_records / batch_size;

    for i in 0..num_batches {
        let start_index = i * batch_size;
        let end_index = (i + 1) * batch_size;

        let block_batch = &blocks[start_index..end_index];
        let transaction_batch = &transactions[start_index..end_index];
        let transfer_batch = &transfers[start_index..end_index];
        let pool_batch = &pools[start_index..end_index];

        // Perform bulk inserts using BinaryCopyInWriter
        let mut writer = BinaryCopyInWriter::new(
            client.copy_in("COPY blocks (block_number, block_hash, parent_hash, block_timestamp, created_at, updated_at) FROM STDIN BINARY").await?,
            &[Type::TEXT, Type::TEXT, Type::TEXT, Type::TIMESTAMP, Type::TIMESTAMP, Type::TIMESTAMP],
        );
        for block in block_batch {
            writer
                .write(&[
                    &block.block_number,
                    &block.block_hash,
                    &block.parent_hash,
                    &block.block_timestamp,
                    &block.created_at,
                    &block.updated_at,
                ])
                .await?;
        }
        writer.finish().await?;

        // Repeat the process for transactions, transfers, and pools
    }

    let bulk_insert_duration = start.elapsed();
    println!("Bulk Insert Test:");
    println!("  Total records: {}", total_records);
    println!("  Batch size: {}", batch_size);
    println!("  Duration: {:?}", bulk_insert_duration);
    println!(
        "  Average insertion rate: {} records/sec",
        total_records as f64 / bulk_insert_duration.as_secs_f64()
    );

    Ok(())
}
